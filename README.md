# ğŸ¤– Local RAG Assistant: AI PC Recommender ğŸš€

This is an **Advanced Retrieval-Augmented Generation (RAG)** system built locally to chat with private documents. Inspired by the Krish Naik series, this version is optimized to run 100% locally on your machine using **Ollama**, saving costs and ensuring data privacy.



---

### ğŸ› ï¸ Tech Stack
* **Framework:** LangChain ğŸ¦œğŸ”—
* **LLM:** Llama 3.2:1b (Local via Ollama) ğŸ¦™
* **Embeddings:** mxbai-embed-large (1024-dim) ğŸ”¢
* **Vector Store:** FAISS (Facebook AI Similarity Search) âš¡
* **Database:** Pickle (Metadata storage) ğŸ’¾
* **Environment:** Python 3.10+ ğŸ

---

### ğŸ“¥ Installation & Setup

1. **Clone the Repository** ğŸ“‚
```bash
git clone https://github.com/Shahryar-Sohail/local-rag/
cd local-rag
```
2. **Create & Activate Virtual Environment** ğŸ¦
```bash
python -m venv .venv
```
# On Windows:
```bash
.venv\Scripts\activate
```
3. **Install Dependencies** ğŸ“¦
```bash
pip install -r requirements.txt
```

4. **Setup Local Models (Ollama)** ğŸ“¥
```bash
ollama pull llama3.2:1b
ollama pull mxbai-embed-large
```

5.ğŸš€ **Running the Project**
To test the backend pipeline and see the AI in action:
```bash
python app.py
```

# âš™ï¸ How It WorksIngestion: 
### 1-PDFs and text files are loaded from the data/ directory.
### 2-Chunking: Documents are split into manageable pieces using RecursiveCharacterTextSplitter.
### 3-Embedding: Each chunk is converted into a 1024-dimensional vector using the mxbai-embed-large model.
### 4-Indexing: Vectors are stored in a FAISS index for high-speed similarity search.
### 5-Retrieval: When a query is made, the system finds the top-$k$ most relevant chunks.
### 6-Generation: Llama 3.2 uses the retrieved context to generate a concise, factual summary.